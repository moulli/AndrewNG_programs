{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hippo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_shape=(2, 2), activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\hippo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(18, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\hippo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\hippo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"linear\", kernel_initializer=\"uniform\")`\n"
     ]
    }
   ],
   "source": [
    "# FROM https://github.com/llSourcell/deep_q_learning/blob/master/03_PlayingAgent.ipynb\n",
    "\n",
    "\n",
    "# INITIALIZATION: libraries, parameters, network...\n",
    "\n",
    "from keras.models import Sequential      # One layer after the other\n",
    "from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs\n",
    "from collections import deque            # For storing moves \n",
    "\n",
    "import numpy as np\n",
    "import gym                                # To train our network\n",
    "env = gym.make('MountainCar-v0')          # Choose game (any in the gym should work)\n",
    "\n",
    "import random     # For sampling batches from the observations\n",
    "\n",
    "\n",
    "# Create network. Input is two consecutive game states, output is Q-values of the possible moves.\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))\n",
    "model.add(Flatten())       # Flatten input so as to have no problems with processing\n",
    "model.add(Dense(18, init='uniform', activation='relu'))\n",
    "model.add(Dense(10, init='uniform', activation='relu'))\n",
    "model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    # Same number of outputs as possible actions\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Parameters\n",
    "D = deque()                                # Register where the actions will be stored\n",
    "\n",
    "observetime = 500                          # Number of timesteps we will be acting on the game and observing results\n",
    "epsilon = 0.7                              # Probability of doing a random move\n",
    "gamma = 0.9                                # Discounted future reward. How much we care about steps further in time\n",
    "mb_size = 500                               # Learning minibatch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observing Finished\n"
     ]
    }
   ],
   "source": [
    "# FIRST STEP: Knowing what each action does (Observing)\n",
    "\n",
    "observation = env.reset()                     # Game begins\n",
    "obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs \n",
    "state = np.stack((obs, obs), axis=1)\n",
    "done = False\n",
    "for t in range(observetime):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = np.random.randint(0, env.action_space.n, size=1)[0]\n",
    "    else:\n",
    "        Q = model.predict(state)          # Q-values predictions\n",
    "        action = np.argmax(Q)             # Move with highest Q-value is the chosen one\n",
    "    observation_new, reward, done, info = env.step(action)     # See state of the game, reward... after performing the action\n",
    "    obs_new = np.expand_dims(observation_new, axis=0)          # (Formatting issues)\n",
    "    state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)     # Update the input with the new state of the game\n",
    "    D.append((state, action, reward, state_new, done))         # 'Remember' action and consequence\n",
    "    state = state_new         # Update state\n",
    "    if done:\n",
    "        env.reset()           # Restart game if it's finished\n",
    "        obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs \n",
    "        state = np.stack((obs, obs), axis=1)\n",
    "print('Observing Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "# SECOND STEP: Learning from the observations (Experience replay)\n",
    "\n",
    "minibatch = random.sample(D, mb_size)                              # Sample some moves\n",
    "\n",
    "inputs_shape = (mb_size,) + state.shape[1:]\n",
    "inputs = np.zeros(inputs_shape)\n",
    "targets = np.zeros((mb_size, env.action_space.n))\n",
    "\n",
    "for i in range(0, mb_size):\n",
    "    state = minibatch[i][0]\n",
    "    action = minibatch[i][1]\n",
    "    reward = minibatch[i][2]\n",
    "    state_new = minibatch[i][3]\n",
    "    done = minibatch[i][4]\n",
    "    \n",
    "# Build Bellman equation for the Q function\n",
    "    inputs[i:i+1] = np.expand_dims(state, axis=0)\n",
    "    targets[i] = model.predict(state)\n",
    "    Q_sa = model.predict(state_new)\n",
    "    \n",
    "    if done:\n",
    "        targets[i, action] = reward\n",
    "    else:\n",
    "        targets[i, action] = reward + gamma * np.max(Q_sa)\n",
    "\n",
    "# Train network to output the Q function\n",
    "    model.train_on_batch(inputs, targets)\n",
    "print('Learning Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game ended! Total reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "# THIRD STEP: Play!\n",
    "\n",
    "observation = env.reset()\n",
    "obs = np.expand_dims(observation, axis=0)\n",
    "state = np.stack((obs, obs), axis=1)\n",
    "done = False\n",
    "tot_reward = 0.0\n",
    "while not done:\n",
    "    env.render()                    # Uncomment to see game running\n",
    "    Q = model.predict(state)        \n",
    "    action = np.argmax(Q)         \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    obs = np.expand_dims(observation, axis=0)\n",
    "    state = np.append(np.expand_dims(obs, axis=0), state[:, :1, :], axis=1)    \n",
    "    tot_reward += reward\n",
    "print('Game ended! Total reward: {}'.format(reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New algorithm that works for real (from https://keon.io/deep-q-learning/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 60\n",
    "outofscreen_malus = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # 1-Compute target, which is the highest expected reward\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            # 2-Replace output taken by action with highest expected value\n",
    "            # Note that then (output - replaced_output) equals (highest_expected_value - actual_value)\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            # By fitting model with self.model.predict(state) and target_f, we actually adapt the weights so that \n",
    "            # the difference between output taken by action and maximum reward is minimized!\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/60, score: 12, e: 1.0\n",
      "WARNING:tensorflow:From C:\\Users\\hippo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "episode: 2/60, score: 24, e: 0.98\n",
      "episode: 3/60, score: 18, e: 0.89\n",
      "episode: 4/60, score: 15, e: 0.83\n",
      "episode: 5/60, score: 14, e: 0.77\n",
      "episode: 6/60, score: 16, e: 0.71\n",
      "episode: 7/60, score: 42, e: 0.58\n",
      "episode: 8/60, score: 13, e: 0.54\n",
      "episode: 9/60, score: 8, e: 0.52\n",
      "episode: 10/60, score: 7, e: 0.5\n",
      "episode: 11/60, score: 14, e: 0.47\n",
      "episode: 12/60, score: 10, e: 0.44\n",
      "episode: 13/60, score: 8, e: 0.43\n",
      "episode: 14/60, score: 37, e: 0.35\n",
      "episode: 15/60, score: 42, e: 0.29\n",
      "episode: 16/60, score: 32, e: 0.24\n",
      "episode: 17/60, score: 36, e: 0.2\n",
      "episode: 18/60, score: 38, e: 0.17\n",
      "episode: 19/60, score: 30, e: 0.15\n",
      "episode: 20/60, score: 32, e: 0.12\n",
      "episode: 21/60, score: 43, e: 0.1\n",
      "episode: 22/60, score: 69, e: 0.071\n",
      "episode: 23/60, score: 68, e: 0.05\n",
      "episode: 24/60, score: 113, e: 0.028\n",
      "episode: 25/60, score: 141, e: 0.014\n",
      "episode: 26/60, score: 139, e: 0.01\n",
      "episode: 27/60, score: 87, e: 0.01\n",
      "episode: 28/60, score: 236, e: 0.01\n",
      "episode: 29/60, score: 102, e: 0.01\n",
      "episode: 30/60, score: 78, e: 0.01\n",
      "episode: 31/60, score: 101, e: 0.01\n",
      "episode: 32/60, score: 95, e: 0.01\n",
      "episode: 33/60, score: 120, e: 0.01\n",
      "episode: 34/60, score: 118, e: 0.01\n",
      "episode: 35/60, score: 133, e: 0.01\n",
      "episode: 36/60, score: 158, e: 0.01\n",
      "episode: 37/60, score: 151, e: 0.01\n",
      "episode: 38/60, score: 160, e: 0.01\n",
      "episode: 39/60, score: 206, e: 0.01\n",
      "episode: 40/60, score: 10, e: 0.01\n",
      "episode: 41/60, score: 167, e: 0.01\n",
      "episode: 42/60, score: 188, e: 0.01\n",
      "episode: 43/60, score: 180, e: 0.01\n",
      "episode: 44/60, score: 199, e: 0.01\n",
      "episode: 45/60, score: 245, e: 0.01\n",
      "episode: 46/60, score: 308, e: 0.01\n",
      "episode: 47/60, score: 240, e: 0.01\n",
      "episode: 48/60, score: 204, e: 0.01\n",
      "episode: 49/60, score: 178, e: 0.01\n",
      "episode: 50/60, score: 227, e: 0.01\n",
      "episode: 51/60, score: 180, e: 0.01\n",
      "episode: 52/60, score: 114, e: 0.01\n",
      "episode: 53/60, score: 166, e: 0.01\n",
      "episode: 54/60, score: 263, e: 0.01\n",
      "episode: 55/60, score: 269, e: 0.01\n",
      "episode: 56/60, score: 7, e: 0.01\n",
      "episode: 57/60, score: 7, e: 0.01\n",
      "episode: 58/60, score: 34, e: 0.01\n",
      "episode: 59/60, score: 11, e: 0.01\n",
      "episode: 60/60, score: 54, e: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Train algorithm:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    # agent.load(\"./save/cartpole-dqn.h5\")\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -outofscreen_malus\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e+1, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "#agent.save(\"./save/cartpole-dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 154\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "epoch = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    time.sleep(0.001)\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    state = next_state\n",
    "    epoch += 1\n",
    "    if done:\n",
    "        print(\"score: {}\"\n",
    "              .format(epoch))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same code but with different loss, with Huber loss, which is more robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 5\n",
    "outofscreen_malus = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hippo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "episode: 1/5, score: 18, e: 1.0\n",
      "episode: 2/5, score: 27, e: 0.87\n",
      "episode: 3/5, score: 29, e: 0.65\n",
      "episode: 4/5, score: 13, e: 0.57\n",
      "episode: 5/5, score: 23, e: 0.45\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    # agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -outofscreen_malus\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e+1, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 8\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "epoch = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    state = next_state\n",
    "    epoch += 1\n",
    "    if done:\n",
    "        print(\"score: {}\"\n",
    "              .format(epoch))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are going to upgrade the algorithm by implementing a few tips, given by https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/\n",
    "\n",
    "## First of, fixed Q-target (cf Deepmind):\n",
    "As we update network, our Q-target, that is the quality of the selected action is also updated, so we are chasing after a value that is constantly changing. This can lead to oscillations during the training. An option is to have a training network and a target network, initialized to be the same in the beginning. Then at each step, training network is updated while target network stays the same. Periodically, after a defined number of time steps, weights of training network are transfered to target network\n",
    "--> ACTUALLY THIS ONE WAS ALREADY IMPLEMENTED\n",
    "\n",
    "## Double deep Q-networks:\n",
    "At the beginning of the training, false positive are possible due to noisy maximum Q-value. This can lead to over-estimation of Q-value and therefore biased training. We can use a double estmation: the training network evaluates the best action possible, then target network evaluates the Q-value for taking this action at this state\n",
    "\n",
    "## Dueling deep Q-networks:\n",
    "Sometimes there is no real point in computing actions Q-values from a state, because the whole state is shit. A dueling DQN allows to separate the network between a state analysis and an actions analysis before joining these two streams for the final layer. Dueling DQN will intuitively be able to learn which states are shit without having to compute the effects of each action at each state. As we want to be able to identify influence of state and action we will not sum the two, but rather use the following formula: Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "\n",
    "## Prioritized experience replay:\n",
    "During training, experiences are taken uniformly. But this means experiences with a high Q-value difference but in low number are less likely to be taken. We can remedy that by sampling with a probability of being sampled equal to the difference between the Q-values (NB: these differences will be updated but as we use a FIFO deque to store experiences, old differences get rid of after some time so it is not a problem).\n",
    "A few adjustements are made to this difference (https://pemami4911.github.io/paper-summaries/deep-rl/2016/01/26/prioritizing-experience-replay.html), and sampling can be done using an unsorted sum-tree (https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/), which works as a dichotomy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Add, Subtract, Concatenate, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, fixedQ=False, double=False, PER=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # Parameters for better but longer network:\n",
    "        self.fixedQ = fixedQ\n",
    "        self.double = double\n",
    "        self.PER = PER\n",
    "        if self.PER:\n",
    "            self.IS_weight = 0.01\n",
    "            self.IS_weight_rise = 1.05\n",
    "            self.IS_weight_max = 1\n",
    "            self.randomness = 0.4\n",
    "            self.no_null = 0.001\n",
    "        self.memory_Qdiff = deque(maxlen=2000)\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        # 1 - start - Fixed Q-target\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        # 1 - end - Fixed Q-target\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model        \n",
    "        X_input = Input(shape=(self.state_size,))\n",
    "        X = Dense(24, activation='relu')(X_input)\n",
    "        X = Dense(24, activation='relu')(X)\n",
    "        # 3 - start - Dueling DQN\n",
    "        Xv = Dense(24, activation='elu')(X)\n",
    "        Xv = Dense(1, activation='linear')(Xv)\n",
    "        Xv = Concatenate()([Xv for i in range(self.action_size)])\n",
    "        Xa = Dense(24, activation='elu')(X)\n",
    "        Xa = Dense(self.action_size, activation='linear')(Xa)\n",
    "        Xa = Lambda(lambda x: x - K.mean(Xa))(Xa)\n",
    "        X = Add()([Xv, Xa])\n",
    "        # 3 - end - Dueling DQN\n",
    "        model = Model(inputs=X_input, outputs=X)\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # 1 - start - Fixed Q-target\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    # 1 - end - Fixed Q-target\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        # 4 - start - Prioritized experience replay\n",
    "        if self.PER:\n",
    "            target = self.model.predict(state)[0][action]\n",
    "            if done:\n",
    "                Qval = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                if self.double:\n",
    "                    tm = self.model.predict(next_state)[0]\n",
    "                    tm_action = np.argmax(tm)\n",
    "                    t_action = t[tm_action]\n",
    "                else:\n",
    "                    t_action = np.amax(t)\n",
    "                Qval = reward + self.gamma * t_action\n",
    "            self.memory_Qdiff.append(Qval-target)\n",
    "        # 4 - end - Prioritized experience replay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    # This next one is for actions after training where we want exploitation and no random due to exploration:\n",
    "    def act_IRL(self, state):\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # 4 - start - Prioritized experience replay\n",
    "        if self.PER:\n",
    "            minibatch = self.sumtree(batch_size)\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "        # 4 - end - Prioritized experience replay\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                if self.double:\n",
    "                    # 2 - start - Double DQN\n",
    "                    tm = self.model.predict(next_state)[0] # Prediction for next state\n",
    "                    tm_action = np.argmax(tm) # Action taken\n",
    "                    t_action = t[tm_action] # Target associated to action taken\n",
    "                    # 2 - end - Double DQN\n",
    "                else:\n",
    "                    t_action = np.amax(t)\n",
    "                target[0][action] = reward + self.gamma * t_action\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "            if not self.fixedQ:\n",
    "                self.update_target_model()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        if self.PER:\n",
    "            if self.IS_weight > self.IS_weight_max:\n",
    "                self.IS_weight = self.IS_weight_max\n",
    "            elif self.IS_weight < self.IS_weight_max:\n",
    "                self.IS_weight *= self.IS_weight_rise\n",
    "            \n",
    "    # 4 - start - Prioritized experience replay\n",
    "    def sumtree(self, batch_size):\n",
    "        # Define probabilities to sample:\n",
    "        array_samp = np.abs(np.array(self.memory_Qdiff))\n",
    "        proba_samp = np.power(array_samp+self.no_null, self.randomness) / np.sum(np.power(array_samp+self.no_null, self.randomness))\n",
    "        proba_samp = np.power(1/(len(proba_samp)*proba_samp), self.IS_weight) * proba_samp\n",
    "        proba_samp = proba_samp / np.sum(proba_samp)\n",
    "        # Unsorted sum-tree to sample:\n",
    "        minibatch = list()\n",
    "        for i in range(batch_size):\n",
    "            rand_samp = random.random()\n",
    "            first_sel = 0\n",
    "            end_sel = len(proba_samp) - 1\n",
    "            middle_sel = int(end_sel/2)\n",
    "            while True:\n",
    "                if np.sum(proba_samp[0:middle_sel+1]) <= rand_samp:\n",
    "                    if np.sum(proba_samp[0:middle_sel+2]) > rand_samp:\n",
    "                        minibatch.append(self.memory[middle_sel+1])\n",
    "                        break\n",
    "                    first_sel = middle_sel\n",
    "                else:\n",
    "                    if rand_samp < proba_samp[0]:\n",
    "                        minibatch.append(self.memory[0])\n",
    "                        break\n",
    "                    elif np.sum(proba_samp[0:middle_sel]) <= rand_samp:\n",
    "                        minibatch.append(self.memory[middle_sel])\n",
    "                        break\n",
    "                    end_sel = middle_sel + 1\n",
    "                middle_sel = int((end_sel-first_sel)/2) + first_sel\n",
    "        return minibatch\n",
    "    # 4 - end - Prioritized experience replay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/50, score: 18, e: 1.0\n",
      "episode: 2/50, score: 25, e: 0.89\n",
      "episode: 3/50, score: 20, e: 0.72\n",
      "episode: 4/50, score: 9, e: 0.66\n",
      "episode: 5/50, score: 40, e: 0.44\n",
      "episode: 6/50, score: 67, e: 0.23\n",
      "episode: 7/50, score: 68, e: 0.11\n",
      "episode: 8/50, score: 94, e: 0.044\n",
      "episode: 9/50, score: 62, e: 0.024\n",
      "episode: 10/50, score: 72, e: 0.012\n",
      "episode: 11/50, score: 89, e: 0.0099\n",
      "episode: 12/50, score: 103, e: 0.0099\n",
      "episode: 13/50, score: 35, e: 0.0099\n",
      "episode: 14/50, score: 38, e: 0.0099\n",
      "episode: 15/50, score: 79, e: 0.0099\n",
      "episode: 16/50, score: 29, e: 0.0099\n",
      "episode: 17/50, score: 51, e: 0.0099\n",
      "episode: 18/50, score: 56, e: 0.0099\n",
      "episode: 19/50, score: 50, e: 0.0099\n",
      "episode: 20/50, score: 120, e: 0.0099\n",
      "episode: 21/50, score: 15, e: 0.0099\n",
      "episode: 22/50, score: 38, e: 0.0099\n",
      "episode: 23/50, score: 27, e: 0.0099\n",
      "episode: 24/50, score: 87, e: 0.0099\n",
      "episode: 25/50, score: 138, e: 0.0099\n",
      "episode: 26/50, score: 144, e: 0.0099\n",
      "episode: 27/50, score: 18, e: 0.0099\n",
      "episode: 28/50, score: 184, e: 0.0099\n",
      "episode: 29/50, score: 44, e: 0.0099\n",
      "episode: 30/50, score: 170, e: 0.0099\n",
      "episode: 31/50, score: 17, e: 0.0099\n",
      "episode: 32/50, score: 141, e: 0.0099\n",
      "episode: 33/50, score: 92, e: 0.0099\n",
      "episode: 34/50, score: 8, e: 0.0099\n",
      "episode: 35/50, score: 65, e: 0.0099\n",
      "episode: 36/50, score: 71, e: 0.0099\n",
      "episode: 37/50, score: 36, e: 0.0099\n",
      "episode: 38/50, score: 14, e: 0.0099\n",
      "episode: 39/50, score: 133, e: 0.0099\n",
      "episode: 40/50, score: 99, e: 0.0099\n",
      "episode: 41/50, score: 148, e: 0.0099\n",
      "episode: 42/50, score: 77, e: 0.0099\n",
      "episode: 43/50, score: 125, e: 0.0099\n",
      "episode: 44/50, score: 183, e: 0.0099\n",
      "episode: 45/50, score: 172, e: 0.0099\n",
      "episode: 46/50, score: 118, e: 0.0099\n",
      "episode: 47/50, score: 217, e: 0.0099\n",
      "episode: 48/50, score: 58, e: 0.0099\n",
      "episode: 49/50, score: 102, e: 0.0099\n",
      "episode: 50/50, score: 112, e: 0.0099\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 50\n",
    "outofscreen_malus = 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size, PER=True)\n",
    "    # agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -outofscreen_malus\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                # 1 - start - Fixed Q-target\n",
    "                agent.update_target_model()\n",
    "                # 1 - end - Fixed Q-target\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e+1, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 89\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "epoch = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "    action = agent.act_IRL(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    state = next_state\n",
    "    epoch += 1\n",
    "    if done:\n",
    "        print(\"score: {}\"\n",
    "              .format(epoch))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.4990888, 54.68012  , 29.258003 ]], dtype=float32), array([[-26.979982 ,  26.201048 ,   0.7789326]], dtype=float32)]\n",
      "[array([[ 0.5203731 ,  0.251148  , -0.5022273 ],\n",
      "       [ 0.05490851,  0.49811053,  0.23003411],\n",
      "       [ 0.4512136 , -0.46179175, -0.67568207]], dtype=float32), array([0., 0., 0.], dtype=float32)]\n",
      "[ 1.49908853 54.6801188  29.25800467]\n",
      "[ 1.49908853 54.6801188  29.25800467] [-26.97998214  26.20104814   0.778934  ]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Add, Subtract, Concatenate, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "X_input = Input(shape=(3,))\n",
    "X1 = Dense(3, activation='relu')(X_input)\n",
    "X2 = Lambda(lambda x: x - K.mean(X1))(X1)\n",
    "model = Model(inputs=X_input, outputs=[X1, X2])\n",
    "Inp = np.array([1, 100, -10])\n",
    "print(model.predict(Inp.reshape(1, 3)))\n",
    "print(model.get_weights())\n",
    "temp = np.dot(Inp, model.get_weights()[0])\n",
    "print(temp)\n",
    "temp = temp * (temp > 0)\n",
    "print(temp, temp - np.mean(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00791717 0.0168565  0.11991364 0.23900601 0.34653985 0.37891076\n",
      " 0.43218354 0.49957527 0.5822315  0.64666872 0.72784883 0.7984135\n",
      " 0.89831963 0.96904642 1.        ]\n",
      "0.5811313679412535\n",
      "8\n",
      "[0.01676241 0.03416264 0.04656094 0.05730124 0.23668796 0.23930462\n",
      " 0.38021629 0.46614756 0.59806309 0.77450875 0.78647649 0.81800521\n",
      " 0.84428393 0.89480457 1.        ]\n",
      "0.23736743406980065\n",
      "5\n",
      "[0.00821581 0.12230818 0.14336769 0.21201671 0.28305316 0.34939459\n",
      " 0.44059203 0.55391343 0.64056189 0.69595006 0.80659201 0.8845502\n",
      " 0.95356537 0.95490499 1.        ]\n",
      "0.926350172264023\n",
      "12\n",
      "[0.05255921 0.12571889 0.28420254 0.31025959 0.33795938 0.36757822\n",
      " 0.52402066 0.55202311 0.59316153 0.60062338 0.73371988 0.74023833\n",
      " 0.79076051 0.94136576 1.        ]\n",
      "0.2044220214094079\n",
      "2\n",
      "[0.02972535 0.16439583 0.20736382 0.24978783 0.33352903 0.39658575\n",
      " 0.47043968 0.48817094 0.49343799 0.56040198 0.68193032 0.76235318\n",
      " 0.90479855 0.9559224  1.        ]\n",
      "0.3549371375461178\n",
      "5\n",
      "[0.05433734 0.12105218 0.16198144 0.23825933 0.2434285  0.35422238\n",
      " 0.41483291 0.54773217 0.61320625 0.65351491 0.69502977 0.83058016\n",
      " 0.90979275 0.99370677 1.        ]\n",
      "0.02282082037545341\n",
      "0\n",
      "[0.02936192 0.05563411 0.14984332 0.26086096 0.30819054 0.42118191\n",
      " 0.44446913 0.48022807 0.6067354  0.69939965 0.77993549 0.81043905\n",
      " 0.92477046 0.94412107 1.        ]\n",
      "0.7881861454853478\n",
      "11\n",
      "[0.07622857 0.11877489 0.15547261 0.2284259  0.30097624 0.41016781\n",
      " 0.47271343 0.50450685 0.65073058 0.65324915 0.68724182 0.7048062\n",
      " 0.820813   0.9574913  1.        ]\n",
      "0.009740527782562514\n",
      "0\n",
      "[0.06742825 0.15107023 0.23722613 0.3082115  0.35433826 0.35466374\n",
      " 0.43480668 0.52333472 0.61090676 0.69745167 0.79809491 0.80701156\n",
      " 0.8980059  0.99249564 1.        ]\n",
      "0.094877892043159\n",
      "1\n",
      "[0.04404438 0.07126224 0.13966082 0.27746232 0.3606     0.4455286\n",
      " 0.57098863 0.59541671 0.61085498 0.72061579 0.74158002 0.81648547\n",
      " 0.8999974  0.96757439 1.        ]\n",
      "0.19069026561016889\n",
      "3\n",
      "[0.12487626 0.24956267 0.28124516 0.31326899 0.35125271 0.42944811\n",
      " 0.43544144 0.56520877 0.58781092 0.66460764 0.69559682 0.71351759\n",
      " 0.85858422 0.89458071 1.        ]\n",
      "0.42111816122013157\n",
      "5\n",
      "[0.10469867 0.13202685 0.249131   0.24921285 0.29788307 0.34675878\n",
      " 0.40804477 0.50750465 0.61567526 0.69278754 0.80587173 0.9171541\n",
      " 0.92798553 0.97811356 1.        ]\n",
      "0.49387536056922643\n",
      "7\n",
      "[0.04052359 0.08548092 0.20426157 0.31730252 0.35703734 0.45155978\n",
      " 0.53519706 0.5791239  0.611557   0.72541197 0.74037734 0.83912869\n",
      " 0.84271484 0.88311143 1.        ]\n",
      "0.37249061004105377\n",
      "5\n",
      "[0.04618621 0.05538685 0.08396026 0.08411955 0.21477324 0.30391409\n",
      " 0.43816199 0.45306758 0.57693636 0.71360982 0.85054012 0.92216336\n",
      " 0.93293701 0.97751723 1.        ]\n",
      "0.07844612027700204\n",
      "2\n",
      "[0.10291057 0.14647679 0.16381855 0.23481562 0.31934096 0.40193155\n",
      " 0.40278942 0.46161554 0.55731791 0.60970538 0.68825638 0.77810745\n",
      " 0.836415   0.90915364 1.        ]\n",
      "0.7587163123680924\n",
      "11\n",
      "[0.09659787 0.12161204 0.17980282 0.22545326 0.34315226 0.39351826\n",
      " 0.50287481 0.5346467  0.55545954 0.56059012 0.5676614  0.67379824\n",
      " 0.76847872 0.86132872 1.        ]\n",
      "0.48360975976142484\n",
      "6\n",
      "[0.01932436 0.09586663 0.16802044 0.20255795 0.29617532 0.42667579\n",
      " 0.42791786 0.52932775 0.62568379 0.63165439 0.67258543 0.6858669\n",
      " 0.7923283  0.90997059 1.        ]\n",
      "0.7276723959126812\n",
      "12\n",
      "[0.00632665 0.0223192  0.13570467 0.24458261 0.28109741 0.38524751\n",
      " 0.48809329 0.58041991 0.68187858 0.76685649 0.77885923 0.8244087\n",
      " 0.9304143  0.97884118 1.        ]\n",
      "0.719829562312851\n",
      "9\n",
      "[0.04428139 0.07202954 0.10637911 0.16478071 0.2028327  0.21176711\n",
      " 0.31018988 0.39274565 0.43490748 0.52141667 0.64622584 0.68216117\n",
      " 0.77143966 0.89817368 1.        ]\n",
      "0.968833088912949\n",
      "14\n",
      "[0.05207915 0.09338025 0.1825042  0.20803813 0.24233385 0.36470493\n",
      " 0.47749262 0.52288787 0.60550982 0.70291341 0.76789652 0.89830739\n",
      " 0.92593909 0.94562513 1.        ]\n",
      "0.7711123475904342\n",
      "11\n",
      "[0.07328563 0.13070029 0.2120004  0.27764384 0.40361697 0.40855807\n",
      " 0.43417614 0.54286409 0.57100751 0.70513793 0.82427184 0.87007046\n",
      " 0.94240301 0.9893886  1.        ]\n",
      "0.5940064955633085\n",
      "9\n",
      "[0.01024654 0.01212628 0.06311718 0.18171059 0.30754136 0.35674501\n",
      " 0.45996579 0.47290423 0.54639245 0.62535684 0.70429723 0.82631851\n",
      " 0.93028291 0.98988151 1.        ]\n",
      "0.4733552103671689\n",
      "8\n",
      "[0.11344881 0.2174913  0.29537741 0.33325281 0.38837256 0.40484857\n",
      " 0.46078183 0.57019238 0.67484641 0.69336746 0.70815016 0.71241625\n",
      " 0.83318694 0.89054566 1.        ]\n",
      "0.1251804777222597\n",
      "1\n",
      "[0.11876996 0.17498228 0.25983458 0.33875706 0.43700779 0.51654212\n",
      " 0.53328854 0.54560736 0.67210576 0.71911271 0.74151364 0.81680223\n",
      " 0.9237272  0.94081045 1.        ]\n",
      "0.061701514325570694\n",
      "0\n",
      "[0.05692697 0.14605718 0.18700668 0.26251563 0.3111827  0.31939158\n",
      " 0.40265696 0.49549853 0.53727079 0.61824696 0.63890756 0.7048963\n",
      " 0.77782156 0.89310339 1.        ]\n",
      "0.9538975775561988\n",
      "14\n",
      "[0.02827883 0.14583288 0.19302016 0.31944842 0.42461079 0.50442079\n",
      " 0.52899304 0.57934754 0.62740608 0.69502491 0.80834907 0.88872937\n",
      " 0.98291558 0.98352239 1.        ]\n",
      "0.5515978251606944\n",
      "7\n",
      "[0.1008367  0.20168681 0.26125301 0.34449058 0.35442444 0.43483399\n",
      " 0.49218099 0.59423603 0.62818256 0.63475995 0.6976837  0.79136882\n",
      " 0.86852594 0.9023622  1.        ]\n",
      "0.08591130385308998\n",
      "0\n",
      "[0.01802242 0.07640389 0.14214185 0.15804374 0.30295669 0.42100216\n",
      " 0.44789553 0.46497579 0.49719727 0.63820121 0.71967574 0.76376732\n",
      " 0.78778769 0.92389307 1.        ]\n",
      "0.6231585263158448\n",
      "9\n",
      "[0.02405953 0.0894933  0.12781778 0.23291318 0.37790466 0.41090353\n",
      " 0.48251709 0.57396952 0.61599099 0.71926561 0.87297488 0.8930341\n",
      " 0.91709071 0.98601882 1.        ]\n",
      "0.6776639193892429\n",
      "9\n",
      "[0.06233854 0.16149811 0.22619614 0.23875631 0.35062419 0.46641868\n",
      " 0.53560287 0.59417675 0.63299569 0.74813933 0.8226207  0.86768864\n",
      " 0.88340801 0.99089205 1.        ]\n",
      "0.935244644978143\n",
      "13\n",
      "[0.00100975 0.10376422 0.23068661 0.2309441  0.31501882 0.37671494\n",
      " 0.50569617 0.57439231 0.58822338 0.72821145 0.82253525 0.88320506\n",
      " 0.91879484 0.96426852 1.        ]\n",
      "0.03813161399660381\n",
      "1\n",
      "[0.08378614 0.18390406 0.19250318 0.27645033 0.3844409  0.49224719\n",
      " 0.57883693 0.61970327 0.65214373 0.7319784  0.82398847 0.87097979\n",
      " 0.92985899 0.96574132 1.        ]\n",
      "0.8940681658394118\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for i in range(batch_size):\n",
    "    proba_samp = np.random.rand(15)\n",
    "    proba_samp = proba_samp / np.sum(proba_samp)\n",
    "    # Unsorted sum-tree to sample:\n",
    "    print(np.cumsum(proba_samp))\n",
    "    rand_samp = random.random()\n",
    "    print(rand_samp)\n",
    "    first_sel = 0\n",
    "    end_sel = len(proba_samp) - 1\n",
    "    middle_sel = int(end_sel/2)\n",
    "    while True:\n",
    "        if np.sum(proba_samp[0:middle_sel+1]) <= rand_samp:\n",
    "            if np.sum(proba_samp[0:middle_sel+2]) > rand_samp:\n",
    "                print(middle_sel+1)\n",
    "                break\n",
    "            first_sel = middle_sel\n",
    "        elif np.sum(proba_samp[0:middle_sel+1]) > rand_samp:\n",
    "            if proba_samp[0] > rand_samp:\n",
    "                print(0)\n",
    "                break\n",
    "            elif np.sum(proba_samp[0:middle_sel]) <= rand_samp:\n",
    "                print(middle_sel)\n",
    "                break\n",
    "            end_sel = middle_sel + 1\n",
    "        middle_sel = int((end_sel-first_sel)/2) + first_sel\n",
    "        #print(first_sel, middle_sel, end_sel, rand_samp)\n",
    "        #print(np.cumsum(proba_samp)[middle_sel-1:middle_sel+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
